{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:04:14.361126Z","iopub.execute_input":"2025-07-02T06:04:14.361300Z","iopub.status.idle":"2025-07-02T06:04:30.614152Z","shell.execute_reply.started":"2025-07-02T06:04:14.361283Z","shell.execute_reply":"2025-07-02T06:04:30.613372Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting transformers\n  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.53.0-py3-none-any.whl (10.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\nSuccessfully installed transformers-4.53.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:04:53.822510Z","iopub.execute_input":"2025-07-02T06:04:53.822794Z","iopub.status.idle":"2025-07-02T06:05:29.755615Z","shell.execute_reply.started":"2025-07-02T06:04:53.822762Z","shell.execute_reply":"2025-07-02T06:05:29.754844Z"}},"outputs":[{"name":"stderr","text":"2025-07-02 06:05:09.867402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751436310.311730      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751436310.433753      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pipe = pipeline(\"image-text-to-text\", model = \"Qwen/Qwen2.5-VL-3B-Instruct\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:05:37.273828Z","iopub.execute_input":"2025-07-02T06:05:37.274528Z","iopub.status.idle":"2025-07-02T06:06:40.333306Z","shell.execute_reply.started":"2025-07-02T06:05:37.274503Z","shell.execute_reply":"2025-07-02T06:06:40.332680Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d49285810149088076051b480f1efb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6fea91747ed4bd88c43feef0c499aa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b681c3621504406fb91f38ce3f590549"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb6b9c82db341698ac6e4e8ca0ab53d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5043034f294afab7882df7cdbb1f99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"753db53c61804864b3eef466fe674af5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17176f977fe44e1fbf6103a0617a1881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"342e1a231226430fba3592be7e3e41d5"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beea7a7e88f34dfe8c84463b672fa49d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79022c57b1104ddcaf708767ef824d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbfecc519c6f41d2a25b4ad255ae9019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee027869a7894eeb92571ce075d11906"}},"metadata":{}},{"name":"stderr","text":"You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4b3cf5a83ec48cebfe5133b1fa3ed52"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:10:22.536956Z","iopub.execute_input":"2025-07-02T06:10:22.537711Z","iopub.status.idle":"2025-07-02T06:10:22.542557Z","shell.execute_reply.started":"2025-07-02T06:10:22.537686Z","shell.execute_reply":"2025-07-02T06:10:22.541984Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<transformers.pipelines.image_text_to_text.ImageTextToTextPipeline at 0x7b021647b410>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Hello, how are you?\"}\n        ]\n    },\n]\noutput = pipe(text = messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:11:05.947147Z","iopub.execute_input":"2025-07-02T06:11:05.947751Z","iopub.status.idle":"2025-07-02T06:11:09.095364Z","shell.execute_reply.started":"2025-07-02T06:11:05.947725Z","shell.execute_reply":"2025-07-02T06:11:09.094585Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"output[0]['generated_text'][1]['content']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:12:58.218574Z","iopub.execute_input":"2025-07-02T06:12:58.219367Z","iopub.status.idle":"2025-07-02T06:12:58.224611Z","shell.execute_reply.started":"2025-07-02T06:12:58.219336Z","shell.execute_reply":"2025-07-02T06:12:58.223813Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"\"Hello! I'm just a computer program, so I don't have feelings or emotions like humans do\""},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"def chat_with_model():\n    input_chat = \"Previous Logs: \\n\"\n    while True:\n        input_ = input(\"Enter your message: \")\n        if input_.lower() == \"bye\":\n            print(\"Bye!!!\")\n            break\n        elif input_.lower() == \"refresh\":\n            print(\"Removing previous logs!!\")\n            input_chat = \"Previous Logs: \\n\"\n            continue\n        elif input_.lower() == \"history\":\n            print(\"Chats till now with the model\")\n            print(input_chat)\n            continue\n        new_message = input_chat + \"----\\nNew Message from user (you have to answer the user query and while considering the past messages): \" + input_\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": new_message}\n                ]\n            },\n        ] \n    \n        llm_output = pipe(text = messages, max_new_tokens = 1000)\n        # print(\"Raw Model output: \", llm_output)\n        response = llm_output[0]['generated_text'][1]['content']\n        print(\"Model: \", response)\n        input_chat += \"\\nuser: \"+input_+\"\\nassistant: \"+response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:36:39.436626Z","iopub.execute_input":"2025-07-02T06:36:39.437137Z","iopub.status.idle":"2025-07-02T06:36:39.442531Z","shell.execute_reply.started":"2025-07-02T06:36:39.437115Z","shell.execute_reply":"2025-07-02T06:36:39.441840Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"chat_with_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:23:32.741782Z","iopub.execute_input":"2025-07-02T06:23:32.742054Z","iopub.status.idle":"2025-07-02T06:25:00.242177Z","shell.execute_reply.started":"2025-07-02T06:23:32.742034Z","shell.execute_reply":"2025-07-02T06:25:00.241597Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your message:  Hello\n"},{"name":"stdout","text":"Model:  Hello! How can I assist you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you please tell me about the Transformers architecture used in llms?\n"},{"name":"stdout","text":"Model:  Certainly! The Transformers architecture is a type of neural network that has become very popular in natural language processing\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you tell me about different sections in that architecture?\n"},{"name":"stdout","text":"Model:  Certainly! The Transformers architecture, developed by researchers at Google, consists of several key components:\n\n1.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  refresh\n"},{"name":"stdout","text":"Removing previous logs!!\nModel:  I'm sorry, but I don't understand what you mean by \"refresh.\" Could you please provide\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  bye\n"},{"name":"stdout","text":"Bye!!!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"chat_with_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:25:57.481973Z","iopub.execute_input":"2025-07-02T06:25:57.482231Z","iopub.status.idle":"2025-07-02T06:32:33.932542Z","shell.execute_reply.started":"2025-07-02T06:25:57.482209Z","shell.execute_reply":"2025-07-02T06:32:33.931878Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your message:  Hello\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n----\\nNew Message from user (you have to answer the user query and while considering the past messages): Hello'}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n----\\nNew Message from user (you have to answer the user query and while considering the past messages): Hello'}]}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}]}]\nModel:  Hello! How can I assist you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you please tell me about the Transformers architecture used in llms?\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n\\nuser: Hello\\nassistant: Hello! How can I assist you today?----\\nNew Message from user (you have to answer the user query and while considering the past messages): Can you please tell me about the Transformers architecture used in llms?'}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n\\nuser: Hello\\nassistant: Hello! How can I assist you today?----\\nNew Message from user (you have to answer the user query and while considering the past messages): Can you please tell me about the Transformers architecture used in llms?'}]}, {'role': 'assistant', 'content': 'Certainly! The Transformers architecture is a type of neural network that has become very popular in natural language processing'}]}]\nModel:  Certainly! The Transformers architecture is a type of neural network that has become very popular in natural language processing\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you tell me about different sections in that architecture?\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n\\nuser: Hello\\nassistant: Hello! How can I assist you today?\\nuser: Can you please tell me about the Transformers architecture used in llms?\\nassistant: Certainly! The Transformers architecture is a type of neural network that has become very popular in natural language processing----\\nNew Message from user (you have to answer the user query and while considering the past messages): Can you tell me about different sections in that architecture?'}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n\\nuser: Hello\\nassistant: Hello! How can I assist you today?\\nuser: Can you please tell me about the Transformers architecture used in llms?\\nassistant: Certainly! The Transformers architecture is a type of neural network that has become very popular in natural language processing----\\nNew Message from user (you have to answer the user query and while considering the past messages): Can you tell me about different sections in that architecture?'}]}, {'role': 'assistant', 'content': 'Certainly! The Transformers architecture, developed by researchers at Google, consists of several key components:\\n\\n1.'}]}]\nModel:  Certainly! The Transformers architecture, developed by researchers at Google, consists of several key components:\n\n1.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  okay can you finish what you were saying?\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n\\nuser: Hello\\nassistant: Hello! How can I assist you today?\\nuser: Can you please tell me about the Transformers architecture used in llms?\\nassistant: Certainly! The Transformers architecture is a type of neural network that has become very popular in natural language processing\\nuser: Can you tell me about different sections in that architecture?\\nassistant: Certainly! The Transformers architecture, developed by researchers at Google, consists of several key components:\\n\\n1.----\\nNew Message from user (you have to answer the user query and while considering the past messages): okay can you finish what you were saying?'}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n\\nuser: Hello\\nassistant: Hello! How can I assist you today?\\nuser: Can you please tell me about the Transformers architecture used in llms?\\nassistant: Certainly! The Transformers architecture is a type of neural network that has become very popular in natural language processing\\nuser: Can you tell me about different sections in that architecture?\\nassistant: Certainly! The Transformers architecture, developed by researchers at Google, consists of several key components:\\n\\n1.----\\nNew Message from user (you have to answer the user query and while considering the past messages): okay can you finish what you were saying?'}]}, {'role': 'assistant', 'content': 'Certainly! The Transformers architecture, developed by researchers at Google, consists of several key components:\\n\\n1.'}]}]\nModel:  Certainly! The Transformers architecture, developed by researchers at Google, consists of several key components:\n\n1.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  refresh\n"},{"name":"stdout","text":"Removing previous logs!!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you please tell me about different sections in the transformers model?\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n----\\nNew Message from user (you have to answer the user query and while considering the past messages): Can you please tell me about different sections in the transformers model?'}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Previous Logs: \\n----\\nNew Message from user (you have to answer the user query and while considering the past messages): Can you please tell me about different sections in the transformers model?'}]}, {'role': 'assistant', 'content': \"Sure, I'd be happy to help! The Transformers model is made up of several different components that\"}]}]\nModel:  Sure, I'd be happy to help! The Transformers model is made up of several different components that\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  components that like?\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that----\\nNew Message from user (you have to answer the user query and while considering the past messages): components that like?\"}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that----\\nNew Message from user (you have to answer the user query and while considering the past messages): components that like?\"}]}, {'role': 'assistant', 'content': 'The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes'}]}]\nModel:  The Transformers model is made up of several different components that include:\n\n1. Encoder: This component takes\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  you didn't finish bro\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\\nuser: components that like?\\nassistant: The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes----\\nNew Message from user (you have to answer the user query and while considering the past messages): you didn't finish bro\"}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\\nuser: components that like?\\nassistant: The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes----\\nNew Message from user (you have to answer the user query and while considering the past messages): you didn't finish bro\"}]}, {'role': 'assistant', 'content': \"I'm sorry for the oversight. Let's continue with the explanation:\\n\\n2. Decoder: This component\"}]}]\nModel:  I'm sorry for the oversight. Let's continue with the explanation:\n\n2. Decoder: This component\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  what is Encoder?\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\\nuser: components that like?\\nassistant: The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes\\nuser: you didn't finish bro\\nassistant: I'm sorry for the oversight. Let's continue with the explanation:\\n\\n2. Decoder: This component----\\nNew Message from user (you have to answer the user query and while considering the past messages): what is Encoder?\"}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\\nuser: components that like?\\nassistant: The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes\\nuser: you didn't finish bro\\nassistant: I'm sorry for the oversight. Let's continue with the explanation:\\n\\n2. Decoder: This component----\\nNew Message from user (you have to answer the user query and while considering the past messages): what is Encoder?\"}]}, {'role': 'assistant', 'content': 'The Encoder in the Transformers model is responsible for processing input data and generating an output representation. It typically'}]}]\nModel:  The Encoder in the Transformers model is responsible for processing input data and generating an output representation. It typically\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  history\n"},{"name":"stdout","text":"Chats till now with the model\nPrevious Logs: \n\nuser: Can you please tell me about different sections in the transformers model?\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\nuser: components that like?\nassistant: The Transformers model is made up of several different components that include:\n\n1. Encoder: This component takes\nuser: you didn't finish bro\nassistant: I'm sorry for the oversight. Let's continue with the explanation:\n\n2. Decoder: This component\nuser: what is Encoder?\nassistant: The Encoder in the Transformers model is responsible for processing input data and generating an output representation. It typically\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  What is your current token length?\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\\nuser: components that like?\\nassistant: The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes\\nuser: you didn't finish bro\\nassistant: I'm sorry for the oversight. Let's continue with the explanation:\\n\\n2. Decoder: This component\\nuser: what is Encoder?\\nassistant: The Encoder in the Transformers model is responsible for processing input data and generating an output representation. It typically----\\nNew Message from user (you have to answer the user query and while considering the past messages): What is your current token length?\"}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\\nuser: components that like?\\nassistant: The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes\\nuser: you didn't finish bro\\nassistant: I'm sorry for the oversight. Let's continue with the explanation:\\n\\n2. Decoder: This component\\nuser: what is Encoder?\\nassistant: The Encoder in the Transformers model is responsible for processing input data and generating an output representation. It typically----\\nNew Message from user (you have to answer the user query and while considering the past messages): What is your current token length?\"}]}, {'role': 'assistant', 'content': 'I apologize for the confusion earlier. To clarify, the current token length refers to the number of tokens'}]}]\nModel:  I apologize for the confusion earlier. To clarify, the current token length refers to the number of tokens\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  What is it though?\n"},{"name":"stdout","text":"Raw Model output:  [{'input_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\\nuser: components that like?\\nassistant: The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes\\nuser: you didn't finish bro\\nassistant: I'm sorry for the oversight. Let's continue with the explanation:\\n\\n2. Decoder: This component\\nuser: what is Encoder?\\nassistant: The Encoder in the Transformers model is responsible for processing input data and generating an output representation. It typically\\nuser: What is your current token length?\\nassistant: I apologize for the confusion earlier. To clarify, the current token length refers to the number of tokens----\\nNew Message from user (you have to answer the user query and while considering the past messages): What is it though?\"}]}], 'generated_text': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Previous Logs: \\n\\nuser: Can you please tell me about different sections in the transformers model?\\nassistant: Sure, I'd be happy to help! The Transformers model is made up of several different components that\\nuser: components that like?\\nassistant: The Transformers model is made up of several different components that include:\\n\\n1. Encoder: This component takes\\nuser: you didn't finish bro\\nassistant: I'm sorry for the oversight. Let's continue with the explanation:\\n\\n2. Decoder: This component\\nuser: what is Encoder?\\nassistant: The Encoder in the Transformers model is responsible for processing input data and generating an output representation. It typically\\nuser: What is your current token length?\\nassistant: I apologize for the confusion earlier. To clarify, the current token length refers to the number of tokens----\\nNew Message from user (you have to answer the user query and while considering the past messages): What is it though?\"}]}, {'role': 'assistant', 'content': 'The current token length refers to the number of tokens in the input sequence that the model is working on'}]}]\nModel:  The current token length refers to the number of tokens in the input sequence that the model is working on\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  bye\n"},{"name":"stdout","text":"Bye!!!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chat_with_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:36:47.587080Z","iopub.execute_input":"2025-07-02T06:36:47.587842Z","iopub.status.idle":"2025-07-02T06:45:32.017157Z","shell.execute_reply.started":"2025-07-02T06:36:47.587815Z","shell.execute_reply":"2025-07-02T06:45:32.016603Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you please tell me about Transformers models?\n"},{"name":"stdout","text":"Model:  Transformers are a type of deep learning model that are used for natural language processing tasks such as translation, summarization, and question answering. They were first introduced by the researchers at Google in 2017 and have since become one of the most popular models in the field of machine learning.\nThe key feature of Transformers is their use of self-attention mechanisms, which allow the model to focus on different parts of the input sequence depending on the task at hand. This allows the model to learn more complex relationships between words and phrases than traditional models, leading to better performance on a wide range of NLP tasks.\nThere are many different types of Transformers, each with its own unique architecture and training approach. Some common examples include BERT, GPT, and T5. These models can be fine-tuned for specific tasks or domains, making them highly adaptable and powerful tools for NLP applications.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you please explain the self-attention in these models like I am a 5 year old?\n"},{"name":"stdout","text":"Model:  Sure! Imagine you're playing with your toys and you want to know what's happening with each toy. You don't need to look at every single toy to figure out what's going on because you can just focus on the ones that are important right now. That's kind of what self-attention does in these models.\n\nIn Transformers, instead of looking at all the words in a sentence, the model only looks at the words that are most important for the task it's trying to do. It's like when you're telling a story and you only focus on the parts that make the most sense to the listener. This way, the model can understand the meaning of the sentence better and do a better job of what it's supposed to do.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you give me an example and explain the exact mechanism?\n"},{"name":"stdout","text":"Model:  Sure! Let's imagine we have a sentence like \"The quick brown fox jumps over the lazy dog.\" In this sentence, we want to understand the meaning of each word and how they relate to each other. \n\nIn a traditional model, we might try to understand the sentence by looking at each word individually. But in a Transformer model, we can use self-attention to focus on the most important parts of the sentence. For example, if we're trying to understand the meaning of the sentence, we might focus on the words \"quick,\" \"brown,\" and \"lazy\" because they describe the characteristics of the animals in the sentence.\n\nTo explain the exact mechanism, let's say we have a sentence like \"The quick brown fox jumps over the lazy dog.\" We can break down the sentence into smaller pieces and then use self-attention to focus on the most important parts. For example, we might focus on the words \"quick,\" \"brown,\" and \"lazy\" because they describe the characteristics of the animals in the sentence. Then, we can use self-attention to focus on the words \"fox\" and \"dog\" because they are the main subjects of the sentence. Finally, we can use self-attention to focus on the words \"jumps\" and \"over\" because they describe the action taking place in the sentence.\n\nBy using self-attention, the model can learn to understand the meaning of the sentence better and do a better job of what it's supposed to do.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  Can you please explain the Query, Key and Value matrices used in this architecture ?\n"},{"name":"stdout","text":"Model:  Sure! Let's break down the Query, Key, and Value matrices used in the Transformer architecture:\n\n### Query Matrix (Q)\nThe Query matrix is used to project the input sequence into a new space where the model can focus on different parts of the sequence. Each element in the Query matrix corresponds to a position in the input sequence. The Query matrix is typically a dense matrix, meaning it has the same number of rows and columns as the input sequence.\n\nFor example, if we have an input sequence of length \\( L \\), the Query matrix will have dimensions \\( L \\times d \\), where \\( d \\) is the dimensionality of the hidden state.\n\n### Key Matrix (K)\nThe Key matrix is also used to project the input sequence into a new space. However, unlike the Query matrix, the Key matrix is symmetric and transposed. This means that the Key matrix has the same number of rows and columns as the input sequence, but it is transposed so that the elements are arranged in a way that allows the model to compare each element in the sequence with every other element.\n\nThe Key matrix is typically a dense matrix, with dimensions \\( L \\times L \\).\n\n### Value Matrix (V)\nThe Value matrix is used to store the actual values of the input sequence. Each element in the Value matrix corresponds to a position in the input sequence. The Value matrix is typically a dense matrix, with dimensions \\( L \\times d \\), where \\( d \\) is the dimensionality of the hidden state.\n\n### Self-Attention Mechanism\nThe self-attention mechanism uses the Query, Key, and Value matrices to compute attention scores. The attention scores determine how much weight each element in the input sequence should be given in the output sequence.\n\nThe attention score for each element \\( i \\) in the input sequence is calculated as follows:\n\\[ \\text{Attention}(i) = \\frac{\\text{Query}_i \\cdot \\text{Key}_i^T}{\\sqrt{d}} \\]\n\nHere, \\( \\text{Query}_i \\) and \\( \\text{Key}_i \\) are the \\( i \\)-th row of the Query and Key matrices, respectively. The dot product \\( \\text{Query}_i \\cdot \\text{Key}_i^T \\) computes the similarity between the Query and Key vectors. Dividing by \\( \\sqrt{d} \\) normalizes the scores.\n\nThe output of the self-attention mechanism is then computed by multiplying the attention scores with the corresponding elements in the Value matrix:\n\\[ \\text{Output}(i) = \\text{Attention}(i) \\cdot \\text{Value}_i \\]\n\nThis process is repeated for each element in the input sequence, resulting in a new sequence of outputs that captures the relationships between different parts of the input sequence.\n\n### Summary\n- **Query Matrix (Q)**: Projects the input sequence into a new space.\n- **Key Matrix (K)**: Projects the input sequence into a symmetric and transposed space.\n- **Value Matrix (V)**: Stores the actual values of the input sequence.\n\nThese matrices are combined through the self-attention mechanism to compute attention scores and produce the final output sequence.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message:  bye\n"},{"name":"stdout","text":"Bye!!!\n","output_type":"stream"}],"execution_count":22}]}